{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "import os\n",
    "import csv\n",
    "from io import StringIO\n",
    "import json\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RDD transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lines:['# Apache Spark', '', 'Spark is a unified analytics engine for large-scale data processing. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 'supports general computation graphs for data analysis. It also supports a', 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 'MLlib for machine learning, GraphX for graph processing,', 'and Structured Streaming for stream processing.', '', '<https://spark.apache.org/>']\n",
      "\n",
      "nums:[1, 2, 3, 3]\n",
      "\n",
      "map:[('#', '# Apache Spark'), ('', ''), ('Spark', 'Spark is a unified analytics engine for large-scale data processing. It provides'), ('high-level', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'), ('supports', 'supports general computation graphs for data analysis. It also supports a'), ('rich', 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,'), ('MLlib', 'MLlib for machine learning, GraphX for graph processing,'), ('and', 'and Structured Streaming for stream processing.'), ('', ''), ('<https://spark.apache.org/>', '<https://spark.apache.org/>')]\n",
      "\n",
      "flatMap:[1, 2, 3, 2, 3, 3, 3]\n",
      "\n",
      "filter:['high-level APIs in Scala, Java, Python, and R, and an optimized engine that', '## Interactive Python Shell', 'Alternatively, if you prefer Python, you can use the Python shell:']\n",
      "\n",
      "distinct:[1, 2, 3]\n",
      "\n",
      "sample:[1, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "lines = sc.textFile('README.md')\n",
    "nums = sc.parallelize([1, 2, 3, 3])\n",
    "items = ['lines','nums','map','flatMap','filter','distinct','sample']\n",
    "map = lines.map(lambda x: (x.split(\" \")[0], x))\n",
    "flatMap = nums.flatMap(lambda x: list(range(x, 4)))\n",
    "filter = lines.filter(lambda line: \"Python\" in line)\n",
    "distinct = nums.distinct()\n",
    "sample = nums.sample(False, 0.5)\n",
    "for item in items:\n",
    "    print(f\"\\n{item}:{eval(item).take(10)}\")\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-RDD Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nums:[1, 2, 3, 3]\n",
      "\n",
      "others:[3, 3, 6, 7]\n",
      "\n",
      "union:[1, 2, 3, 3, 3, 3, 6, 7]\n",
      "\n",
      "intersection:[3]\n",
      "\n",
      "subtract:[2, 1]\n",
      "\n",
      "cartesian:[(1, 3), (1, 3), (1, 6), (1, 7), (2, 3), (2, 3), (2, 6), (2, 7), (3, 3), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "nums = sc.parallelize([1, 2, 3, 3])\n",
    "others = sc.parallelize([3, 3, 6, 7])\n",
    "items = ['nums','others','union','intersection','subtract','cartesian']\n",
    "union = nums.union(others)\n",
    "intersection = nums.intersection(others)\n",
    "subtract = nums.subtract(others)\n",
    "cartesian = nums.cartesian(others)\n",
    "for item in items:\n",
    "    print(f\"\\n{item}:{eval(item).take(10)}\")\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic action on an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rdd:ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262\n",
      "\n",
      "collect:[1, 2, 3, 3]\n",
      "\n",
      "count:4\n",
      "\n",
      "countByValue:dict_items([(1, 1), (2, 1), (3, 2)])\n",
      "\n",
      "take:[1, 2]\n",
      "\n",
      "top:[3, 3]\n",
      "\n",
      "takeOrdered:[1, 2]\n",
      "\n",
      "takeSample:[1]\n",
      "\n",
      "reduce:9\n",
      "\n",
      "fold:9\n",
      "\n",
      "aggregate:(9, 4)\n",
      "\n",
      "foreach:None\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "rdd = sc.parallelize([1, 2, 3, 3])\n",
    "items =['rdd','collect','count','countByValue','take','top','takeOrdered','takeSample',\n",
    "        'reduce','fold','aggregate','foreach']\n",
    "collect = rdd.collect()\n",
    "count = rdd.count()\n",
    "countByValue = rdd.countByValue().items()\n",
    "take = rdd.take(2)\n",
    "top = rdd.top(2)\n",
    "takeOrdered = rdd.takeOrdered(2)\n",
    "takeSample = rdd.takeSample(False, 1)\n",
    "reduce = rdd.reduce(lambda x, y: x + y)\n",
    "fold = rdd.fold(0, lambda x, y: x + y)\n",
    "aggregate = rdd.aggregate((0, 0),\n",
    "                          (lambda acc, value: (acc[0] + value, acc[1] + 1)),\n",
    "                          (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    "                          )\n",
    "def squared(x):\n",
    "    print(x)\n",
    "foreach = rdd.foreach(squared)\n",
    "for item in items:\n",
    "    print(f\"\\n{item}:{eval(item)}\")\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation on one pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pairs:[(1, 2), (3, 4), (3, 6)]\n",
      "\n",
      "reduceByKey:[(1, 2), (3, 10)]\n",
      "\n",
      "groupByKey:[(1, [2]), (3, [4, 6])]\n",
      "\n",
      "combineByKey:[(1, 2.0), (3, 5.0)]\n",
      "\n",
      "mapValues:[(1, 3), (3, 5), (3, 7)]\n",
      "\n",
      "flatMapValues:[(1, 2), (1, 3), (1, 4), (1, 5), (3, 4), (3, 5)]\n",
      "\n",
      "keys:[1, 3, 3]\n",
      "\n",
      "values:[2, 4, 6]\n",
      "\n",
      "sortByKey:[(1, 2), (3, 4), (3, 6)]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "pairs = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "items = ['pairs','reduceByKey', 'groupByKey', 'combineByKey',\n",
    "         'mapValues', 'flatMapValues', 'keys', 'values', 'sortByKey']\n",
    "reduceByKey = pairs.reduceByKey(lambda x, y: x + y)\n",
    "groupByKey = pairs.groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "sumCount = pairs.combineByKey((lambda x: (x, 1)),\n",
    "                                  (lambda x, y: (x[0] + y, x[1] + 1)),\n",
    "                                  (lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "combineByKey = sumCount.map(lambda key_xy: (key_xy[0], key_xy[1][0]/key_xy[1][1]))\n",
    "mapValues = pairs.mapValues(lambda x: x + 1)\n",
    "flatMapValues = pairs.flatMapValues(lambda x: list(range(x, 6)))\n",
    "keys = pairs.keys()\n",
    "values = pairs.values()\n",
    "sortByKey = pairs.sortByKey()\n",
    "for item in items:\n",
    "    print(f\"\\n{item}:{eval(item).collect()}\")\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on two pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pair1:[(1, 2), (3, 4), (3, 6)]\n",
      "\n",
      "pair2:[(3, 9)]\n",
      "\n",
      "subtractByKey:[(1, 2)]\n",
      "\n",
      "join:[(3, (4, 9)), (3, (6, 9))]\n",
      "\n",
      "rightOuterJoin:[(3, (4, 9)), (3, (6, 9))]\n",
      "\n",
      "leftOuterJoin:[(1, (2, None)), (3, (4, 9)), (3, (6, 9))]\n",
      "\n",
      "cogroup:[(1, ([2], [])), (3, ([4, 6], [9]))]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "pair1 = sc.parallelize([(1,2),(3,4),(3,6)])\n",
    "pair2 = sc.parallelize([(3,9)])\n",
    "items = ['pair1', 'pair2', 'subtractByKey', 'join',\n",
    "         'rightOuterJoin', 'leftOuterJoin', 'cogroup']\n",
    "subtractByKey = pair1.subtractByKey(pair2)\n",
    "join = pair1.join(pair2)\n",
    "rightOuterJoin = pair1.rightOuterJoin(pair2)\n",
    "leftOuterJoin = pair1.leftOuterJoin(pair2)\n",
    "cogroup = pair1.cogroup(pair2).map(lambda x: (x[0], (list(x[1][0]), list(x[1][1]))))\n",
    "for item in items:\n",
    "    print(f\"\\n{item}:{eval(item).collect()}\")\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions on pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pairs:ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262\n",
      "\n",
      "countByKey:dict_items([(1, 1), (3, 2)])\n",
      "\n",
      "collectAsMap:{1: 2, 3: 6}\n",
      "\n",
      "lookup:[4, 6]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "pairs = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "items = ['pairs','countByKey', 'collectAsMap','lookup']\n",
    "countByKey = pairs.countByKey().items()\n",
    "collectAsMap = pairs.collectAsMap()\n",
    "lookup = pairs.lookup(3)\n",
    "for item in items:\n",
    "    print(f\"\\n{item}:{eval(item)}\")\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv.txt:[{'name': '1 Kiyeon', 'favoriteAnimal': None}, {'name': '2 Chungyeol', 'favoriteAnimal': None}]\n",
      "csv.txt:[{'name': '1 Kiyeon', 'favoriteAnimal': None}, {'name': '2 Chungyeol', 'favoriteAnimal': None}]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "input = sc.textFile('input/pandas.txt')\n",
    "data = input.map(lambda x: json.loads(x))\n",
    "if not os.path.isdir('output/lovespandas'):\n",
    "    data.filter(lambda x: x['lovesPandas']).map(lambda x: json.dumps(x)).saveAsTextFile('output/lovespandas')\n",
    "\n",
    "def loadRecord(line):\n",
    "    input = StringIO(line)\n",
    "    reader = csv.DictReader(input, fieldnames=['name','favoriteAnimal'])\n",
    "    return next(reader)\n",
    "input = sc.textFile('input/csv.txt').map(loadRecord)\n",
    "print(f'csv.txt:{input.collect()}')\n",
    "def loadRecords(fileNameContents):\n",
    "    input = StringIO(fileNameContents[1])\n",
    "    reader = csv.DictReader(input, fieldnames=[\"name\", \"favoriteAnimal\"])\n",
    "    return reader\n",
    "fullFileData = sc.wholeTextFiles('input/csv.txt').flatMap(loadRecords)\n",
    "print(f'csv.txt:{fullFileData.collect()}')\n",
    "\n",
    "def writeRecords(records):\n",
    "    output = StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=[\"name\", \"favoriteAnimal\"])\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]\n",
    "\n",
    "if not os.path.isdir('output/csv'):\n",
    "    fullFileData.mapPartitions(writeRecords).saveAsTextFile('output/csv')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sql table with txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "myTable query: [Row(key=238, value='val_238'), Row(key=86, value='val_86'), Row(key=311, value='val_311'), Row(key=27, value='val_27'), Row(key=165, value='val_165'), Row(key=409, value='val_409'), Row(key=255, value='val_255'), Row(key=278, value='val_278'), Row(key=98, value='val_98'), Row(key=484, value='val_484')]\n",
      "\n",
      "myTable key: [238, 86, 311, 27, 165, 409, 255, 278, 98, 484]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "url = 'https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/kv1.txt'\n",
    "inputFile = 'input/kv1.txt'\n",
    "inputTable = 'myTable'\n",
    "if not os.path.isfile(inputFile):\n",
    "    wget.download(url, inputFile)\n",
    "hiveCtx = HiveContext(sc)\n",
    "if not os.path.isdir(f'spark-warehouse/{inputTable}'):\n",
    "    hiveCtx.sql(f\"CREATE TABLE IF NOT EXISTS {inputTable} (key INT, value STRING)\")\n",
    "    hiveCtx.sql(f\"LOAD DATA LOCAL INPATH '{inputFile}' INTO TABLE myTable\")\n",
    "input = hiveCtx.sql(f\"FROM {inputTable} SELECT key, value\")\n",
    "print(f'\\nmyTable query: {input.rdd.take(10)}')\n",
    "print(f'\\nmyTable key: {input.rdd.map(lambda row: row[0]).take(10)}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read json file into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tweets query: [Row(name=None, text=None), Row(name='Holden', text='Nice day out today'), Row(name='Matei', text='Ever nicer here :)'), Row(name=None, text=None)]\n",
      "\n",
      "tweets text: [None, 'Nice day out today', 'Ever nicer here :)', None]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "hiveCtx = HiveContext(sc)\n",
    "tweets = hiveCtx.read.json('input/tweets.json')\n",
    "tweets.registerTempTable(\"tweets\")\n",
    "results = hiveCtx.sql(\"SELECT user.name, text FROM tweets\")\n",
    "resultsText = results.rdd.map(lambda row: row.text)\n",
    "print(f'\\ntweets query: {results.collect()}')\n",
    "print(f'\\ntweets text: {resultsText.collect()}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
